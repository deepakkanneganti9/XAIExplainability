Infodemic events such as COVID-19, wars, and elections present significant challenges for governing bodies in managing the rapid spread of misinformation on social media. Existing misinformation detection approaches leverage advanced Natural Language Processing (NLP) techniques such as hybrid transfer learning models (e.g., BERT variants), but they often lack explainability to ensure fairness and interoperability. To address this challenge, we propose a novel Explainable Hybrid Transfer Learning model for infodemic misinformation detection. Specifically, we integrate two pre-trained transformer models, BERT and BART, within a hybrid transfer learning architecture to capture both contextual understanding and generative linguistic patterns. We embed explainable AI (XAI) techniques, including SHAP and ELI5, as plug-and-play modules to interpret model predictions without altering the underlying architecture. This enables the identification of influential linguistic features that contribute to misinformation classification. Extensive experiments on multiple real-world infodemic datasets demonstrate that the proposed model consistently outperforms standard transfer learning baselines while achieving a balanced trade-off between detection accuracy and explainability.
