Infodemic events such as pandemics, wars, and elections accelerate the spread of misinformation on social media, demanding accurate and transparent detection models. While hybrid transfer learning architectures based on pre-trained language models (e.g., BERT and BART) achieve strong predictive performance, their multi-stage design prevents the effective use of standard explainable artificial intelligence (XAI) techniques. In particular, the lack of compatible inference mechanisms limits model interpretability in high-stakes decision-making settings. This paper proposes an explainable hybrid transfer learning framework that addresses this inference gap. We introduce a predict--probable inference mechanism that enables plug-and-play integration of widely used post hoc XAI techniques, including SHAP and ELI5, without modifying the underlying model architecture. Experiments on multiple real-world infodemic datasets show that the proposed approach outperforms state-of-the-art baselines while achieving a balanced trade-off between detection accuracy and explainability.
